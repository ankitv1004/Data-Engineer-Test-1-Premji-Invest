{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to fetch and parse the data\n",
    "def fetch_data(url):\n",
    "    try:\n",
    "        # Fetch the webpage content\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "       #     print(response)\n",
    "\n",
    "        # Parse the HTML content\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        print(soup)\n",
    "        \n",
    "        # Example: Get the title of the page\n",
    "        title = soup.title.string if soup.title else \"No Title Found\"\n",
    "        print(title)\n",
    "        \n",
    "        # Example: Get the first paragraph or some other content\n",
    "        paragraph = soup.find('p').get_text() if soup.find('p') else \"No Paragraph Found\"\n",
    "        \n",
    "        return {\"url\": url, \"title\": title, \"paragraph\": paragraph}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "# URLs to scrape\n",
    "urls = [  \"https://yourstory.com/search?q=HDFC\", \"https://backend.finshots.in/backend/search/?q=HDFC\"]\n",
    "\n",
    "# Fetch data from both URLs\n",
    "for url in urls:\n",
    "    data = fetch_data(url)\n",
    "    print(f\"Data from {data['url']}:\")\n",
    "    print(f\"Title: {data['title']}\")\n",
    "    print(f\"First Paragraph: {data['paragraph']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch and parse the data\n",
    "\n",
    "def fetch_data():\n",
    "    df=pd.DataFrame(columns=['title','post_url','published_date','article'])\n",
    "    # URLs to scrape\n",
    "    keywords=[\"HDFC\",\"Tata Motars\"]\n",
    "    # Fetch data from both URLs\n",
    "    data=[]\n",
    "\n",
    "    for keyword in keywords:\n",
    "        url = f\"https://backend.finshots.in/backend/search/?q={keyword}\"\n",
    "        try:\n",
    "            # Fetch the webpage content\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Check for request errors\n",
    "            #print(response.text)\n",
    "\n",
    "            # Parse the HTML content\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            soup=json.loads(response.content)\n",
    "            matches=soup[\"matches\"]\n",
    "            #print(matches[0])\n",
    "\n",
    "            for match in matches:\n",
    "                # Example: Get the title of the page\n",
    "                title = match['title']\n",
    "                #print(title)\n",
    "                \n",
    "                # url = match['post_url']\n",
    "                post_url=match['post_url']\n",
    "                published_date=match['published_date']\n",
    "                req = requests.get(url=post_url)\n",
    "                soup = BeautifulSoup(req.content, 'lxml')\n",
    "                # Example: Get the first paragraph or some other content\n",
    "                paragraph = soup.find('div',{\"class\":\"post-content\"}).text.strip()#.get_text() \n",
    "                #paragraph=soup.text.strip()\n",
    "                row=pd.DataFrame({ \"title\": [title],\"post_url\": [post_url],'published_date':[published_date], \"article\": [paragraph]})\n",
    "                #print(row)\n",
    "                df=df._append(row,ignore_index=True)\n",
    "            data.append(df)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"url\": url, \"error\": str(e)}\n",
    "\n",
    "    return data\n",
    "\n",
    "data=fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HDFC is in trouble — Has the elephant forgotte...</td>\n",
       "      <td>https://finshots.in/markets/hdfc-is-in-trouble/</td>\n",
       "      <td>2024-01-19T18:38:03.109000+00:00</td>\n",
       "      <td>In today's Finshots, we explain why HDFC Bank'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45 years later, HDFC bids goodbye to Dalal Street</td>\n",
       "      <td>https://finshots.in/archive/45-years-later-hdf...</td>\n",
       "      <td>2023-07-14T01:30:00+00:00</td>\n",
       "      <td>On 12th July at 3:30 pm, the stock ticker HDFC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HDFC bids farewell to its ₹10,000 crore educat...</td>\n",
       "      <td>https://finshots.in/archive/hdfc-bids-farewell...</td>\n",
       "      <td>2023-06-22T01:30:00+00:00</td>\n",
       "      <td>In today’s Finshots, we explain why internatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 investing lessons from HDFC’s Prashant Jain</td>\n",
       "      <td>https://finshots.in/markets/investing-lessons-...</td>\n",
       "      <td>2022-09-09T10:46:32+00:00</td>\n",
       "      <td>In today's Finshots we describe a few lessons ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekly Wrapup - Understanding the HDFC Merger</td>\n",
       "      <td>https://finshots.in/archive/weekly-wrapup-unde...</td>\n",
       "      <td>2022-04-08T16:51:54.934000+00:00</td>\n",
       "      <td>Before we get to today's story, a quick recap ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "6  HDFC is in trouble — Has the elephant forgotte...   \n",
       "8  45 years later, HDFC bids goodbye to Dalal Street   \n",
       "5  HDFC bids farewell to its ₹10,000 crore educat...   \n",
       "7      3 investing lessons from HDFC’s Prashant Jain   \n",
       "3      Weekly Wrapup - Understanding the HDFC Merger   \n",
       "\n",
       "                                            post_url  \\\n",
       "6    https://finshots.in/markets/hdfc-is-in-trouble/   \n",
       "8  https://finshots.in/archive/45-years-later-hdf...   \n",
       "5  https://finshots.in/archive/hdfc-bids-farewell...   \n",
       "7  https://finshots.in/markets/investing-lessons-...   \n",
       "3  https://finshots.in/archive/weekly-wrapup-unde...   \n",
       "\n",
       "                     published_date  \\\n",
       "6  2024-01-19T18:38:03.109000+00:00   \n",
       "8         2023-07-14T01:30:00+00:00   \n",
       "5         2023-06-22T01:30:00+00:00   \n",
       "7         2022-09-09T10:46:32+00:00   \n",
       "3  2022-04-08T16:51:54.934000+00:00   \n",
       "\n",
       "                                             article  \n",
       "6  In today's Finshots, we explain why HDFC Bank'...  \n",
       "8  On 12th July at 3:30 pm, the stock ticker HDFC...  \n",
       "5  In today’s Finshots, we explain why internatio...  \n",
       "7  In today's Finshots we describe a few lessons ...  \n",
       "3  Before we get to today's story, a quick recap ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "    for i in range(0,len(data)):\n",
    "        # Sorting by column \"published_date\"\n",
    "        data[i]=data[i].sort_values(by=['published_date'], ascending=False)\n",
    "        data[i]=data[i].iloc[0:5,:]\n",
    "        #print(data[i])\n",
    "    return data\n",
    "\n",
    "clean_data(data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_url</th>\n",
       "      <th>published_date</th>\n",
       "      <th>article</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HDFC is in trouble — Has the elephant forgotte...</td>\n",
       "      <td>https://finshots.in/markets/hdfc-is-in-trouble/</td>\n",
       "      <td>2024-01-19T18:38:03.109000+00:00</td>\n",
       "      <td>In today's Finshots, we explain why HDFC Bank'...</td>\n",
       "      <td>0.159779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45 years later, HDFC bids goodbye to Dalal Street</td>\n",
       "      <td>https://finshots.in/archive/45-years-later-hdf...</td>\n",
       "      <td>2023-07-14T01:30:00+00:00</td>\n",
       "      <td>On 12th July at 3:30 pm, the stock ticker HDFC...</td>\n",
       "      <td>0.632368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HDFC bids farewell to its ₹10,000 crore educat...</td>\n",
       "      <td>https://finshots.in/archive/hdfc-bids-farewell...</td>\n",
       "      <td>2023-06-22T01:30:00+00:00</td>\n",
       "      <td>In today’s Finshots, we explain why internatio...</td>\n",
       "      <td>0.426931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3 investing lessons from HDFC’s Prashant Jain</td>\n",
       "      <td>https://finshots.in/markets/investing-lessons-...</td>\n",
       "      <td>2022-09-09T10:46:32+00:00</td>\n",
       "      <td>In today's Finshots we describe a few lessons ...</td>\n",
       "      <td>0.757981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weekly Wrapup - Understanding the HDFC Merger</td>\n",
       "      <td>https://finshots.in/archive/weekly-wrapup-unde...</td>\n",
       "      <td>2022-04-08T16:51:54.934000+00:00</td>\n",
       "      <td>Before we get to today's story, a quick recap ...</td>\n",
       "      <td>0.832144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "6  HDFC is in trouble — Has the elephant forgotte...   \n",
       "8  45 years later, HDFC bids goodbye to Dalal Street   \n",
       "5  HDFC bids farewell to its ₹10,000 crore educat...   \n",
       "7      3 investing lessons from HDFC’s Prashant Jain   \n",
       "3      Weekly Wrapup - Understanding the HDFC Merger   \n",
       "\n",
       "                                            post_url  \\\n",
       "6    https://finshots.in/markets/hdfc-is-in-trouble/   \n",
       "8  https://finshots.in/archive/45-years-later-hdf...   \n",
       "5  https://finshots.in/archive/hdfc-bids-farewell...   \n",
       "7  https://finshots.in/markets/investing-lessons-...   \n",
       "3  https://finshots.in/archive/weekly-wrapup-unde...   \n",
       "\n",
       "                     published_date  \\\n",
       "6  2024-01-19T18:38:03.109000+00:00   \n",
       "8         2023-07-14T01:30:00+00:00   \n",
       "5         2023-06-22T01:30:00+00:00   \n",
       "7         2022-09-09T10:46:32+00:00   \n",
       "3  2022-04-08T16:51:54.934000+00:00   \n",
       "\n",
       "                                             article  sentiment_score  \n",
       "6  In today's Finshots, we explain why HDFC Bank'...         0.159779  \n",
       "8  On 12th July at 3:30 pm, the stock ticker HDFC...         0.632368  \n",
       "5  In today’s Finshots, we explain why internatio...         0.426931  \n",
       "7  In today's Finshots we describe a few lessons ...         0.757981  \n",
       "3  Before we get to today's story, a quick recap ...         0.832144  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_sentiment_score(data):\n",
    "    import random\n",
    "    def mock_sentiment_api(text):\n",
    "        sentiment_score = random.uniform(0, 1)\n",
    "        return sentiment_score \n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "        data[i]['sentiment_score'] = data[i].apply(mock_sentiment_api, axis=1)\n",
    "    return data\n",
    "generate_sentiment_score(data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "keys=[\"dd\",\"ee\"]\n",
    "b={\"ddd\":1,'dee':2}\n",
    "for i in range(0,len(keys)):\n",
    "    a=b[f\"d{keys[i]}\"]\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import hashlib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_articles():\n",
    "    # Code to scrape articles from finshots.in for keywords HDFC and Tata Motors\n",
    "    query = [\"HDFC\",\"Tata Motors\"]\n",
    "\n",
    "    article_df_final = pd.DataFrame(columns=['aid','source','type','title','url','publishDate','text','query'])\n",
    "    for each_query in query:\n",
    "        article_df = pd.DataFrame(columns=['aid','source','type','title','url','publishDate','text'])\n",
    "        r = requests.get(url=\"https://backend.finshots.in/backend/search/?q=\"+each_query)\n",
    "        out = json.loads(r.content)\n",
    "        print(out)\n",
    "        matches = out['matches']\n",
    "        for match in matches:\n",
    "            title = match['title']\n",
    "            publishDate = datetime.datetime.strptime(match['published_date'].split('T')[0],\"%Y-%m-%d\").date().strftime('%Y-%m-%d')\n",
    "            url = match['post_url']\n",
    "            req = requests.get(url=url)\n",
    "            #print(req.text)\n",
    "            soup = BeautifulSoup(req.content, 'lxml')\n",
    "            p_tags = soup.find_all('div',{\"class\":\"post-content\"})\n",
    "            text = \"\"\n",
    "            hash_obj = hashlib.sha256(url.encode('utf-8'))\n",
    "            hex_hash = hash_obj.hexdigest()\n",
    "            aid = str(hex_hash)\n",
    "            for p in p_tags:\n",
    "                text+=p.text\n",
    "            article_df = pd.concat([article_df, pd.DataFrame([[aid,'FinShots','Article',title,url,publishDate,text,each_query]],columns=['aid','source','type','title','url','publishDate','text','query'])],ignore_index=True)\n",
    "        # Taking first five records\n",
    "        article_df.sort_values(by='publishDate', ascending=False, inplace=True)\n",
    "        first_five_records = article_df.head(5)\n",
    "        article_df_final = pd.concat([article_df_final,first_five_records],ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return article_df_final\n",
    "\n",
    "fetch_articles().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dags/scripts/movie_analysis/ml-100k/u.user\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame(columns=['user_id', 'age', 'gender', 'occupation', 'zip'])\n",
    "file = open(\"D:/airflow/dags/ml-100k/u.user\", \"r\")\n",
    "for each_row in file:\n",
    "    user_df = pd.concat([user_df, pd.DataFrame([each_row.split('|')],\\\n",
    "                                                   columns=['user_id', 'age', 'gender', 'occupation', 'zip'])],\\\n",
    "                              ignore_index=True)\n",
    "user_df['zip'] = [each_zip.rstrip(\"\\n\") for each_zip in user_df['zip']]\n",
    "user_df['age'] = [int(age) for age in user_df['age']]\n",
    "\n",
    "# Reading ratings data\n",
    "rating_df= pd.read_csv(\"D:/airflow/dags/ml-100k/u.data\", delimiter=\"\\t\", names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "# Reading item data\n",
    "item_columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action',\n",
    "                    'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                    'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "item_data = pd.DataFrame(columns=item_columns)\n",
    "genres = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', 'Documentary', 'Drama',\n",
    "              'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War',\n",
    "              'Western']\n",
    "item_file = open(\"D:/airflow/dags/ml-100k/u.item\", \"r\", encoding='iso-8859-1')\n",
    "for x in item_file:\n",
    "    item_data = pd.concat([item_data, pd.DataFrame([x.split('|')], columns=item_columns)], ignore_index=True)\n",
    "item_data['Western'] = [x.rstrip(\"\\n\") for x in item_data['Western']]\n",
    "item_data['movie_id'] = [int(x) for x in item_data['movie_id']]\n",
    "for genre in genres:\n",
    "    item_data[genre] = [int(x) for x in item_data[genre]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       occupation        age\n",
      "0   administrator  38.746835\n",
      "1          artist  31.392857\n",
      "2          doctor  43.571429\n",
      "3        educator  42.010526\n",
      "4        engineer  36.388060\n",
      "5   entertainment  29.222222\n",
      "6       executive  38.718750\n",
      "7      healthcare  41.562500\n",
      "8       homemaker  32.571429\n",
      "9          lawyer  36.750000\n",
      "10      librarian  40.000000\n",
      "11      marketing  37.615385\n",
      "12           none  26.555556\n",
      "13          other  34.523810\n",
      "14     programmer  33.121212\n",
      "15        retired  63.071429\n",
      "16       salesman  35.666667\n",
      "17      scientist  35.548387\n",
      "18        student  22.081633\n",
      "19     technician  33.148148\n",
      "20         writer  36.311111\n"
     ]
    }
   ],
   "source": [
    "# Grouping by 'occupation' and calculating the mean age\n",
    "mean_age_by_occupation = user_df.groupby('occupation')['age'].mean().reset_index()\n",
    "\n",
    "# Display the result\n",
    "print(mean_age_by_occupation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           movie_title  avg_rating  \\\n",
      "407                              Close Shave, A (1995)    4.491071   \n",
      "317                            Schindler's List (1993)    4.466443   \n",
      "168                         Wrong Trousers, The (1993)    4.466102   \n",
      "482                                  Casablanca (1942)    4.456790   \n",
      "113  Wallace & Gromit: The Best of Aardman Animatio...    4.447761   \n",
      "63                    Shawshank Redemption, The (1994)    4.445230   \n",
      "602                                 Rear Window (1954)    4.387560   \n",
      "11                          Usual Suspects, The (1995)    4.385768   \n",
      "49                                    Star Wars (1977)    4.358491   \n",
      "177                                12 Angry Men (1957)    4.344000   \n",
      "512                              Third Man, The (1949)    4.333333   \n",
      "133                                Citizen Kane (1941)    4.292929   \n",
      "962            Some Folks Call It a Sling Blade (1993)    4.292683   \n",
      "426                       To Kill a Mockingbird (1962)    4.292237   \n",
      "356             One Flew Over the Cuckoo's Nest (1975)    4.291667   \n",
      "97                    Silence of the Lambs, The (1991)    4.289744   \n",
      "479                          North by Northwest (1959)    4.284916   \n",
      "126                              Godfather, The (1972)    4.283293   \n",
      "284                              Secrets & Lies (1996)    4.265432   \n",
      "271                           Good Will Hunting (1997)    4.262626   \n",
      "\n",
      "     rating_count  \n",
      "407           112  \n",
      "317           298  \n",
      "168           118  \n",
      "482           243  \n",
      "113            67  \n",
      "63            283  \n",
      "602           209  \n",
      "11            267  \n",
      "49            583  \n",
      "177           125  \n",
      "512            72  \n",
      "133           198  \n",
      "962            41  \n",
      "426           219  \n",
      "356           264  \n",
      "97            390  \n",
      "479           179  \n",
      "126           413  \n",
      "284           162  \n",
      "271           198  \n"
     ]
    }
   ],
   "source": [
    "# Merge rating_df with item_data to get movie titles\n",
    "merged_df = pd.merge(rating_df, item_data, left_on='item_id', right_on='movie_id')\n",
    "\n",
    "# Group by movie and calculate count of ratings and mean rating\n",
    "movie_ratings = merged_df.groupby(['movie_id', 'movie_title']).agg(\n",
    "    rating_count=('rating', 'count'),\n",
    "    avg_rating=('rating', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Filter movies that have been rated at least 35 times\n",
    "filtered_movies = movie_ratings[movie_ratings['rating_count'] >= 35]\n",
    "\n",
    "# Sort by average rating in descending order and select top 20\n",
    "top_20_movies = filtered_movies.sort_values(by='avg_rating', ascending=False).head(20)\n",
    "\n",
    "# Display the top 20 movies\n",
    "print(top_20_movies[['movie_title', 'avg_rating', 'rating_count']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         occupation age_group  genre  total_ratings\n",
      "7     administrator     20-25  Drama            376\n",
      "26    administrator     25-35  Drama           3512\n",
      "45    administrator     35-45  Drama           3672\n",
      "64    administrator       45+  Drama           4204\n",
      "83           artist     20-25  Drama            568\n",
      "...             ...       ...    ...            ...\n",
      "1508     technician       45+  Drama            982\n",
      "1527         writer     20-25  Drama            782\n",
      "1546         writer     25-35  Drama           3019\n",
      "1565         writer     35-45  Drama           2246\n",
      "1584         writer       45+  Drama           1546\n",
      "\n",
      "[84 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_12088\\1688052622.py:30: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  genre_rating_summary = genre_ratings.groupby(['occupation', 'age_group', 'genre']).agg(\n",
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_12088\\1688052622.py:36: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  genre_rating_summary.groupby(['occupation', 'age_group'])['total_ratings'].idxmax()\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'user_id' and 'item_id' are of the same type in all DataFrames\n",
    "user_df['user_id'] = user_df['user_id'].astype(int)\n",
    "rating_df['user_id'] = rating_df['user_id'].astype(int)\n",
    "rating_df['item_id'] = rating_df['item_id'].astype(int)\n",
    "item_data['movie_id'] = item_data['movie_id'].astype(int)\n",
    "\n",
    "# 1. Define age groups in user_df\n",
    "age_bins = [20, 25, 35, 45, 100]  # Age groups: 20-25, 25-35, 35-45, 45+\n",
    "age_labels = ['20-25', '25-35', '35-45', '45+']\n",
    "user_df['age_group'] = pd.cut(user_df['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# 2. Merge user_df, rating_df, and item_data on 'user_id' and 'item_id' (movie_id)\n",
    "merged_df = pd.merge(rating_df, user_df, on='user_id')\n",
    "merged_df = pd.merge(merged_df, item_data, left_on='item_id', right_on='movie_id')\n",
    "\n",
    "# 3. Group by occupation, age_group, and genres\n",
    "# List of genre columns in item_data\n",
    "genres = ['unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime', \n",
    "          'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', \n",
    "          'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "# Melt the dataframe to count ratings for each genre\n",
    "genre_ratings = merged_df.melt(id_vars=['occupation', 'age_group', 'rating'], \n",
    "                               value_vars=genres, var_name='genre', value_name='is_genre')\n",
    "\n",
    "# Filter rows where 'is_genre' is 1 (indicating the movie belongs to that genre)\n",
    "genre_ratings = genre_ratings[genre_ratings['is_genre'] == 1]\n",
    "\n",
    "# 4. Aggregate ratings by occupation, age group, and genre, and sum the ratings\n",
    "genre_rating_summary = genre_ratings.groupby(['occupation', 'age_group', 'genre']).agg(\n",
    "    total_ratings=('rating', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# 5. For each occupation and age group, find the genre with the highest total rating\n",
    "top_genres_by_occupation_age = genre_rating_summary.loc[\n",
    "    genre_rating_summary.groupby(['occupation', 'age_group'])['total_ratings'].idxmax()\n",
    "]\n",
    "\n",
    "# Display the top genres for each occupation and age group\n",
    "print(top_genres_by_occupation_age[['occupation', 'age_group', 'genre', 'total_ratings']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 similar movies:\n",
      "Movie ID: 928, Similarity: 0.5491, Co-occurrence: 82\n",
      "Movie ID: 969, Similarity: 0.4631, Co-occurrence: 58\n",
      "Movie ID: 926, Similarity: 0.4580, Co-occurrence: 86\n",
      "Movie ID: 378, Similarity: 0.4563, Co-occurrence: 74\n",
      "Movie ID: 326, Similarity: 0.4548, Co-occurrence: 62\n",
      "Movie ID: 588, Similarity: 0.4430, Co-occurrence: 166\n",
      "Movie ID: 627, Similarity: 0.4375, Co-occurrence: 63\n",
      "Movie ID: 72, Similarity: 0.4329, Co-occurrence: 100\n",
      "Movie ID: 959, Similarity: 0.4284, Co-occurrence: 50\n",
      "Movie ID: 418, Similarity: 0.4284, Co-occurrence: 105\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create a User-Movie Ratings Matrix\n",
    "# Pivot rating_df to create user-movie matrix\n",
    "user_movie_matrix = rating_df.pivot(index='user_id', columns='item_id', values='rating')\n",
    "\n",
    "# 2. Calculate Similarity Between Movies\n",
    "# Create a movie-to-movie similarity dictionary\n",
    "def calculate_similarity(movie_id, movie_ratings_matrix, threshold=50):\n",
    "    similar_movies = []\n",
    "    target_movie_ratings = movie_ratings_matrix[movie_id]\n",
    "    \n",
    "    for other_movie_id in movie_ratings_matrix.columns:\n",
    "        if other_movie_id == movie_id:\n",
    "            continue\n",
    "            \n",
    "        # Get common users who rated both movies\n",
    "        common_users = target_movie_ratings.dropna().index.intersection(\n",
    "            movie_ratings_matrix[other_movie_id].dropna().index)\n",
    "        \n",
    "        # Only consider movies with enough co-occurrence\n",
    "        if len(common_users) >= 50:\n",
    "            # Calculate Pearson correlation for similarity\n",
    "            similarity, _ = pearsonr(\n",
    "                movie_ratings_matrix[movie_id].loc[common_users],\n",
    "                movie_ratings_matrix[other_movie_id].loc[common_users]\n",
    "            )\n",
    "            \n",
    "            # If similarity is greater than or equal to the similarity threshold (0.95)\n",
    "            if similarity >= 0.0:\n",
    "                similar_movies.append((other_movie_id, similarity, len(common_users)))\n",
    "    \n",
    "    return similar_movies\n",
    "\n",
    "# 3. Find top 10 similar movies for a given movie\n",
    "def find_top_similar_movies(movie_id, movie_ratings_matrix):\n",
    "    similar_movies = calculate_similarity(movie_id, movie_ratings_matrix)\n",
    "    \n",
    "    # Sort movies by similarity score and co-occurrence count\n",
    "    similar_movies_sorted = sorted(similar_movies, key=lambda x: (-x[1], -x[2]))[:10]\n",
    "    \n",
    "    return similar_movies_sorted\n",
    "\n",
    "# Example: Finding top 10 similar movies to a given movie (say movie_id = 1)\n",
    "movie_id = 1  # Replace with the ID of the movie you're interested in\n",
    "top_similar_movies = find_top_similar_movies(movie_id, user_movie_matrix)\n",
    "\n",
    "# Display the results (movie_id, similarity score, co-occurrence count)\n",
    "print(\"Top 10 similar movies:\")\n",
    "for movie, similarity, co_occurrence in top_similar_movies:\n",
    "    print(f\"Movie ID: {movie}, Similarity: {similarity:.4f}, Co-occurrence: {co_occurrence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
